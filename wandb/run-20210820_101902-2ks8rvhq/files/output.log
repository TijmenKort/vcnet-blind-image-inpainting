
I0820 10:19:12.355446 22062 trainer.py:252] Using 4 GPUs...
W0820 10:19:22.172214 22062 warnings.py:109] /pytorch/aten/src/ATen/native/TensorFactories.cpp:361: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning.
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
tensor(0.1782, device='cuda:0')
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x14ae03c9d8b0>
Traceback (most recent call last):
  File "/home/jlpkort/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 962, in __del__
tensor(0.1782, device='cuda:0')
    self._shutdown_workers()
  File "/home/jlpkort/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 942, in _shutdown_workers
    w.join()
  File "/home/jlpkort/anaconda3/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/jlpkort/anaconda3/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/home/jlpkort/anaconda3/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
KeyboardInterrupt:
tensor(0.1782, device='cuda:0')
Traceback (most recent call last):
  File "main.py", line 52, in <module>
    trainer.run()
  File "/home/jlpkort/vcnet-blind-image-inpainting/engine/trainer.py", line 123, in run
    smooth_masks = self.mask_smoother(1 - masks) + masks
  File "/home/jlpkort/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jlpkort/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 155, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/jlpkort/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 165, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/jlpkort/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 77, in parallel_apply
    thread.join()
  File "/home/jlpkort/anaconda3/lib/python3.8/threading.py", line 1011, in join
    self._wait_for_tstate_lock()
  File "/home/jlpkort/anaconda3/lib/python3.8/threading.py", line 1027, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt